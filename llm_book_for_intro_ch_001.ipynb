{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWNKk5SVwg5EUoATVLNkCK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enginearn/llm_book_for_intro/blob/main/llm_book_for_intro_ch_001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 1"
      ],
      "metadata": {
        "id": "Y2s56jFuljOA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J7YS8bIlgyK",
        "outputId": "87a062dc-65ab-4934-9273-1c82df623109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[ja,sentencepiece,torch] in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepiece,torch]) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepiece,torch]) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepiece,torch]) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepiece,torch]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepiece,torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepiece,torch]) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepiece,torch]) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepiece,torch]) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepiece,torch]) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepiece,torch]) (4.65.0)\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[ja,sentencepiece,torch])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepiece,torch]) (3.20.3)\n",
            "Requirement already satisfied: fugashi>=1.0 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepiece,torch]) (1.2.1)\n",
            "Requirement already satisfied: ipadic<2.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepiece,torch]) (1.0.0)\n",
            "Requirement already satisfied: unidic-lite>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepiece,torch]) (1.0.8)\n",
            "Requirement already satisfied: unidic>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepiece,torch]) (1.1.0)\n",
            "Requirement already satisfied: sudachipy>=0.6.6 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepiece,torch]) (0.6.7)\n",
            "Requirement already satisfied: sudachidict-core>=20220729 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepiece,torch]) (20230110)\n",
            "Requirement already satisfied: rhoknp<1.3.1,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepiece,torch]) (1.3.0)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepiece,torch]) (2.0.1+cu118)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[ja,sentencepiece,torch]) (0.21.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[ja,sentencepiece,torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[ja,sentencepiece,torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[ja,sentencepiece,torch]) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[ja,sentencepiece,torch]) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[ja,sentencepiece,torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[ja,sentencepiece,torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[ja,sentencepiece,torch]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[ja,sentencepiece,torch]) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[ja,sentencepiece,torch]) (16.0.6)\n",
            "Requirement already satisfied: wasabi<1.0.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from unidic>=1.0.2->transformers[ja,sentencepiece,torch]) (0.10.1)\n",
            "Requirement already satisfied: plac<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from unidic>=1.0.2->transformers[ja,sentencepiece,torch]) (1.3.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[ja,sentencepiece,torch]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[ja,sentencepiece,torch]) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[ja,sentencepiece,torch]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[ja,sentencepiece,torch]) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[ja,sentencepiece,torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[ja,sentencepiece,torch]) (1.3.0)\n",
            "Installing collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[ja,sentencepiece,torch]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "huSufkxgl69D"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text classification"
      ],
      "metadata": {
        "id": "tc7MMOtipvfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.tools import text_classification\n",
        "text_classification_pipeline = pipeline(\n",
        "    model=\"llm-book/bert-base-japanese-v3-marc_ja\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjJ_0VjRnCr4",
        "outputId": "aa11ed3b-4738-4ea0-af2c-ab27bb867910"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-marc_ja/snapshots/7b47edf80477fd9da0ee1bc1908326ac012d624f/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"llm-book/bert-base-japanese-v3-marc_ja\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"positive\",\n",
            "    \"1\": \"negative\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"negative\": 1,\n",
            "    \"positive\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32768\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-marc_ja/snapshots/7b47edf80477fd9da0ee1bc1908326ac012d624f/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"llm-book/bert-base-japanese-v3-marc_ja\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"positive\",\n",
            "    \"1\": \"negative\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"negative\": 1,\n",
            "    \"positive\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32768\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-marc_ja/snapshots/7b47edf80477fd9da0ee1bc1908326ac012d624f/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at llm-book/bert-base-japanese-v3-marc_ja.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-marc_ja/snapshots/7b47edf80477fd9da0ee1bc1908326ac012d624f/vocab.txt\n",
            "loading file spiece.model from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-marc_ja/snapshots/7b47edf80477fd9da0ee1bc1908326ac012d624f/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-marc_ja/snapshots/7b47edf80477fd9da0ee1bc1908326ac012d624f/tokenizer_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_text = \"明日は明日の風が吹く\"\n",
        "print(text_classification_pipeline(positive_text)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7Oya6YdpgrY",
        "outputId": "c1ef8e16-a61b-4022-a912-f63a6e55bb74"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': 'positive', 'score': 0.998440682888031}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negative_text = \"お金がなさ過ぎてつらい...\"\n",
        "print(text_classification_pipeline(negative_text)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-tN8W5yoEnE",
        "outputId": "b7c6177d-9b5c-4258-abda-b26c4de33f5b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': 'negative', 'score': 0.9948321580886841}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Natural Language Inference: 自然言語推論"
      ],
      "metadata": {
        "id": "XQbp_V61p_yP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nli_pipline = pipeline(model=\"llm-book/bert-base-japanese-v3-jnli\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vxrCDpPpJuf",
        "outputId": "78d38e81-4b11-49ce-d6d5-46453a09c5de"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-jnli/snapshots/9056fce079ed3fc284c9b2d1c2abccae3d13af61/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"llm-book/bert-base-japanese-v3-jnli\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"entailment\",\n",
            "    \"1\": \"contradiction\",\n",
            "    \"2\": \"neutral\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"contradiction\": 1,\n",
            "    \"entailment\": 0,\n",
            "    \"neutral\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32768\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-jnli/snapshots/9056fce079ed3fc284c9b2d1c2abccae3d13af61/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"llm-book/bert-base-japanese-v3-jnli\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"entailment\",\n",
            "    \"1\": \"contradiction\",\n",
            "    \"2\": \"neutral\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"contradiction\": 1,\n",
            "    \"entailment\": 0,\n",
            "    \"neutral\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32768\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-jnli/snapshots/9056fce079ed3fc284c9b2d1c2abccae3d13af61/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at llm-book/bert-base-japanese-v3-jnli.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-jnli/snapshots/9056fce079ed3fc284c9b2d1c2abccae3d13af61/vocab.txt\n",
            "loading file spiece.model from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-jnli/snapshots/9056fce079ed3fc284c9b2d1c2abccae3d13af61/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-jnli/snapshots/9056fce079ed3fc284c9b2d1c2abccae3d13af61/tokenizer_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`entailment`は`含意`であり、\"二人の女性が山を眺めています\"が成立するならば、\"山を眺めている人が2人います\"も成立するという関係を表す。"
      ],
      "metadata": {
        "id": "nM3AfEmNx9-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"二人の女性が山を眺めています\"\n",
        "entailment_text = \"山を眺めている人が2人います\"\n",
        "\n",
        "print(nli_pipline({\"text\": text, \"text_pair\": entailment_text}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJokfd-lqd8r",
        "outputId": "88f5a361-6ed6-4dd5-84d8-792de2d2fb1f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': 'entailment', 'score': 0.9933545589447021}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`contradiction`は、`矛盾`"
      ],
      "metadata": {
        "id": "LEGVIAsRyunI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contradiction_text = \"女性２人が山を破壊しています\"\n",
        "\n",
        "print(nli_pipline({\"text\": text, \"text_pair\": contradiction_text}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmLN1ZEhwOvJ",
        "outputId": "2b74bbc6-40fe-4d2d-df39-82d4049d3069"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': 'contradiction', 'score': 0.9771778583526611}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neutral_text = \"女性２人が山で料理をしています\"\n",
        "\n",
        "print(nli_pipline({\"text\": text, \"text_pair\": neutral_text}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCLi0Ulxw3c9",
        "outputId": "fb5e42d2-7f94-48ef-e151-f778af9ef449"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': 'neutral', 'score': 0.9970123767852783}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic textual similarity: STS\n",
        "\n",
        "二つのテキストが似ている度合いをスコアとして予測するタスク"
      ],
      "metadata": {
        "id": "s-gGTaGIzYvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_sts_pipeline = pipeline(\n",
        "    model=\"llm-book/bert-base-japanese-v3-jsts\",\n",
        "    function_to_apply=\"none\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aSzRJzrzLmH",
        "outputId": "bf2b7fdc-123c-47c9-fc79-1bb3dd088334"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-jsts/snapshots/01d23e59c46236a19a9171a06cf649a5ebc26a7e/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"llm-book/bert-base-japanese-v3-jsts\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"regression\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32768\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-jsts/snapshots/01d23e59c46236a19a9171a06cf649a5ebc26a7e/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"llm-book/bert-base-japanese-v3-jsts\",\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"regression\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32768\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-jsts/snapshots/01d23e59c46236a19a9171a06cf649a5ebc26a7e/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "All the weights of BertForSequenceClassification were initialized from the model checkpoint at llm-book/bert-base-japanese-v3-jsts.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-jsts/snapshots/01d23e59c46236a19a9171a06cf649a5ebc26a7e/vocab.txt\n",
            "loading file spiece.model from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-jsts/snapshots/01d23e59c46236a19a9171a06cf649a5ebc26a7e/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-jsts/snapshots/01d23e59c46236a19a9171a06cf649a5ebc26a7e/tokenizer_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"マチュピチュにはマヤ文明がありました\"\n",
        "sim_text = \"ストーンヘンジにはドルイド文明が存在していました。\"\n",
        "\n",
        "result = text_sts_pipeline({\"text\": text, \"text_pair\": sim_text})\n",
        "print(result[\"score\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vuK9N1G0jfO",
        "outputId": "6843ef59-7e83-4014-dce2-8e192e55d1d5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0985182523727417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sim_text = \"マチュピチュはマヤ文明の一部でした。\"\n",
        "\n",
        "result = text_sts_pipeline({\"text\": text, \"text_pair\": sim_text})\n",
        "print(result[\"score\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "990Eryma4BD-",
        "outputId": "2e2a2c6e-4dc5-456e-80b0-69955254e5af"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.884174346923828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dissim_text = \"トイレの壁に黒いタオルがかけられています\"\n",
        "\n",
        "result = text_sts_pipeline({\"text\": text, \"text_pair\": dissim_text})\n",
        "print(result[\"score\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqwb8sxp4mQ5",
        "outputId": "8ed3aeb8-6541-4332-8f64-84db703e1e11"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.05538347363471985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import cosine_similarity\n",
        "\n",
        "sim_enc_pipeline = pipeline(\n",
        "    model=\"llm-book/bert-base-japanese-v3-unsup-simcse-jawiki\",\n",
        "    task=\"feature-extraction\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqU6zH024636",
        "outputId": "7df4e1ac-768f-44ba-b953-5bdec2730c62"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-unsup-simcse-jawiki/snapshots/aa5681f6270216673e39e8e56659afb69e93caea/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"llm-book/bert-base-japanese-v3-unsup-simcse-jawiki\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32768\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-unsup-simcse-jawiki/snapshots/aa5681f6270216673e39e8e56659afb69e93caea/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"llm-book/bert-base-japanese-v3-unsup-simcse-jawiki\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32768\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-unsup-simcse-jawiki/snapshots/aa5681f6270216673e39e8e56659afb69e93caea/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing BertModel.\n",
            "\n",
            "All the weights of BertModel were initialized from the model checkpoint at llm-book/bert-base-japanese-v3-unsup-simcse-jawiki.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-unsup-simcse-jawiki/snapshots/aa5681f6270216673e39e8e56659afb69e93caea/vocab.txt\n",
            "loading file spiece.model from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-unsup-simcse-jawiki/snapshots/aa5681f6270216673e39e8e56659afb69e93caea/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--llm-book--bert-base-japanese-v3-unsup-simcse-jawiki/snapshots/aa5681f6270216673e39e8e56659afb69e93caea/tokenizer_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# textとsim_textのベクトルを獲得\n",
        "text_emb = sim_enc_pipeline(text, return_tensors=True)[0][0]\n",
        "sim_emb = sim_enc_pipeline(sim_text, return_tensors=True)[0][0]\n",
        "# textとsim_textの類似度を計算\n",
        "sim_pair_score = cosine_similarity(text_emb, sim_emb, dim=0)\n",
        "print(sim_pair_score.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nwyg4XBy50AL",
        "outputId": "b4382a96-750a-475a-deb6-87552573ac05"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9352689385414124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dissim_textのベクトルを獲得\n",
        "dissim_emb = sim_enc_pipeline(dissim_text, return_tensors=True)[0][0]\n",
        "# textとdissim_textの類似度を計算\n",
        "dissim_pair_score = cosine_similarity(text_emb, dissim_emb, dim=0)\n",
        "print(dissim_pair_score.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYJD2spL8I3w",
        "outputId": "10192451-4faa-4db7-b208-0bde8ef8132a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.33561891317367554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Named entity recognition: NER"
      ],
      "metadata": {
        "id": "3eP5NpVo8fmT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FJB-SxvM8Xti"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}